{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30 Days of Pandas Challenge: Solutions with Concepts Review\n",
    "The 30 days of Pandas challenge features problems in data filtering, string methods, data manipulation, statistics, aggregation, and integration. Each section's problems feature usage of their own distinct pandas library functions and concepts. Preceding each solution is an explanation of any new concepts or functions used, along with explanation of more niche technical details. \n",
    "\n",
    "Creating this notebook and saving it serves as a personal study guide for doing data work with pandas.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 595: or-filtering\n",
    "def big_countries(world: pd.DataFrame) -> pd.DataFrame:\n",
    "    big_countries = world[(world[\"area\"] >= 3000000) | (world[\"population\"] >= 25000000)]\n",
    "    return big_countries[[\"name\",\"population\",\"area\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1757: and-filtering\n",
    "def find_products(products: pd.DataFrame) -> pd.DataFrame:\n",
    "    return products[(products[\"low_fats\"] == \"Y\") & (products[\"recyclable\"] == \"Y\")][[\"product_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 183: isin filtering, negation\n",
    "def find_customers(customers: pd.DataFrame, orders: pd.DataFrame) -> pd.DataFrame:\n",
    "    does_not_order = ~customers[\"id\"].isin(orders[\"customerId\"])\n",
    "    return customers[does_not_order][[\"name\"]].rename(columns={\"name\":\"Customers\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1148: .drop_duplicates(), .sort_values()\n",
    "def article_views(views: pd.DataFrame) -> pd.DataFrame:\n",
    "    self_views = views[views[\"author_id\"] == views[\"viewer_id\"]]\n",
    "    unique_self_views = self_views.drop_duplicates(subset=[\"author_id\", \"viewer_id\"])\n",
    "\n",
    "    return unique_self_views[[\"author_id\"]].rename(columns={\"author_id\": \"id\"}).sort_values(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1683: .len()\n",
    "def invalid_tweets(tweets: pd.DataFrame) -> pd.DataFrame:\n",
    "    return tweets[tweets[\"content\"].str.len() > 15][[\"tweet_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1873: .startswith(), .loc[mask, \"col\"]\n",
    "def calculate_special_bonus(employees: pd.DataFrame) -> pd.DataFrame:\n",
    "    employees[\"bonus\"] = 0\n",
    "    \n",
    "    # access the bonus field of each row that deserves a bonus\n",
    "    # loc is efficient to doing df[deserves_bonus][\"bonus\"], where an intermiate df gets produced. I guess .apply() is still nice too\n",
    "    deserves_bonus = ~(employees[\"name\"].str.startswith(\"M\")) & (employees[\"employee_id\"] % 2 == 1)\n",
    "    employees.loc[deserves_bonus, \"bonus\"] = employees[\"salary\"]\n",
    "    return employees[[\"employee_id\", \"bonus\"]].sort_values(\"employee_id\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1667: .lower(), .capitalize()\n",
    "def fix_names(users: pd.DataFrame) -> pd.DataFrame:\n",
    "    users[\"name\"] = users[\"name\"].str.lower().str.capitalize()\n",
    "    return users.sort_values(\"user_id\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1517: .apply() for more particular string conditions, no regex\n",
    "def valid_prefix(s: str) -> bool:\n",
    "    starts_with_letter = s[0].isalpha()\n",
    "    if not starts_with_letter:\n",
    "        return False\n",
    "    \n",
    "    valid_special_chars = set(\".-_\")\n",
    "    for char in s:\n",
    "        if (char in valid_special_chars) or char.isalpha() or char.isdigit():\n",
    "            continue\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def valid_emails(users: pd.DataFrame) -> pd.DataFrame:\n",
    "    valid_prefix_mask = users[\"mail\"].str.rsplit(\"@\", n=1).str[0].apply(valid_prefix)\n",
    "    valid_domain_mask = users[\"mail\"].str.endswith(\"@leetcode.com\")\n",
    "    return users[valid_prefix_mask & valid_domain_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1527: regex\n",
    "def find_patients(patients: pd.DataFrame) -> pd.DataFrame:\n",
    "    #has_diab1 = patients[\"conditions\"].str.split(' ').apply(lambda l: pd.Series(l).str.startswith(\"DIAB1\").any())\n",
    "    has_diab1 = patients[\"conditions\"].str.contains(r\"\\bDIAB1\\w*\\b\")\n",
    "    return patients[has_diab1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 177: pairing .sort_values() with .iloc[] to access nth highest entry\n",
    "def nth_highest_salary(employee: pd.DataFrame, N: int) -> pd.DataFrame:\n",
    "    # remove duplicate salaries to avoid ranking two duplicate salaries different rankings\n",
    "    employee = employee.drop_duplicates(\"salary\")\n",
    "\n",
    "    if N > len(employee):\n",
    "        return pd.DataFrame({'Nth Highest Salary': [None]})\n",
    "\n",
    "    nth_highest_salary = employee.sort_values(\"salary\", ascending=False)[\"salary\"].iloc[N-1]\n",
    "    return pd.DataFrame({'Nth Highest Salary': [nth_highest_salary]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 176: Jokes on you, we can use .nlargest() to return the n rows containing the nlargest. then use .iloc[] to select the desired 2nd highest\n",
    "def second_highest_salary(employee: pd.DataFrame) -> pd.DataFrame:\n",
    "    employee = employee.drop_duplicates(\"salary\")\n",
    "\n",
    "    if len(employee) < 2:\n",
    "        return pd.DataFrame({\"SecondHighestSalary\": [None]})\n",
    "    \n",
    "    # nlargest returns the rows containing the 1st and 2nd largest salaries\n",
    "    second_highest_salary = employee.nlargest(2, \"salary\").iloc[-1][\"salary\"]\n",
    "    return pd.DataFrame({\"SecondHighestSalary\": [second_highest_salary]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#184: .groupby() and .transform() to attribute group statistics to each entry in original df. could also have been accomplished with a merge as well after groupby.  \n",
    "def department_highest_salary(employee: pd.DataFrame, department: pd.DataFrame) -> pd.DataFrame:\n",
    "    employee[\"dept_highest_salary\"] = employee.groupby(\"departmentId\")[\"salary\"].transform(\"max\")\n",
    "    \n",
    "    is_highest_salary = employee[\"salary\"] == employee[\"dept_highest_salary\"]\n",
    "\n",
    "    employees_w_highest_salaries = pd.merge(employee[is_highest_salary], department, left_on=\"departmentId\", right_on=\"id\").rename(columns={\n",
    "        \"name_y\": \"Department\",\n",
    "        \"name_x\": \"Employee\",\n",
    "        \"salary\": \"Salary\"\n",
    "        })\n",
    "\n",
    "    return employees_w_highest_salaries[[\"Department\", \"Employee\", \"Salary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 178: .rank() and using its \"method\" parameter to deal with tiebreakers\n",
    "def order_scores(scores: pd.DataFrame) -> pd.DataFrame:\n",
    "    scores = scores.sort_values(\"score\", ascending=False)\n",
    "    scores[\"rank\"] = scores[\"score\"].rank(ascending=False, method=\"dense\")\n",
    "    return scores.drop(columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 196: using the \"inplace\" parameter, .drop_duplicates() with \"keep\" parameter to deal with tiebreakers\n",
    "# Modify Person in place\n",
    "def delete_duplicate_emails(person: pd.DataFrame) -> None:\n",
    "    person.sort_values(\"id\", ascending=True, inplace=True)\n",
    "    person.drop_duplicates(subset=[\"email\"], keep=\"first\", inplace=True) # \"first\" to keep the row with smallest id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1795: .melt() to convert wide data to long data, .dropna() to drop rows contaning None\n",
    "def rearrange_products_table(products: pd.DataFrame) -> pd.DataFrame:\n",
    "    # id_vars: the pivot column, \n",
    "    # var_name: the name of the new column holding the \"wide\" columns, \n",
    "    # value_name: since you're migrating the wide columns to its own column under var_name, this value_name will be the name of the new column name for the values. \n",
    "    products_long = products.melt(id_vars=\"product_id\", var_name=\"store\", value_name=\"price\")\n",
    "    products_long_nonulls = products_long.dropna()\n",
    "    return products_long_nonulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2082: .nunique() to count number of distinct entries\n",
    "def count_rich_customers(store: pd.DataFrame) -> pd.DataFrame:\n",
    "    filtered_store = store[store[\"amount\"] > 500]\n",
    "    n_rich_customers = filtered_store[\"customer_id\"].nunique()\n",
    "    \n",
    "    return pd.DataFrame({\"rich_count\": [n_rich_customers]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1173: calculating percentages, using .shape[0] to access number of rows \n",
    "def food_delivery(delivery: pd.DataFrame) -> pd.DataFrame:\n",
    "    is_same_day = delivery[\"order_date\"] == delivery[\"customer_pref_delivery_date\"]\n",
    "    prop_intermediate = round(100 * delivery[is_same_day].shape[0] / delivery.shape[0], 2)\n",
    "    return pd.DataFrame({\"immediate_percentage\": [prop_intermediate]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1907: classification, no new methods\n",
    "def count_salary_categories(accounts: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.DataFrame({\n",
    "        \"category\": [\"Low Salary\", \"Average Salary\", \"High Salary\"],\n",
    "        \"accounts_count\": [\n",
    "            accounts[ accounts[\"income\"] < 20000 ].shape[0],\n",
    "            accounts[ (20000 <= accounts[\"income\"]) & (accounts[\"income\"] <= 50000) ].shape[0],\n",
    "            accounts[ 50000 < accounts[\"income\"] ].shape[0]\n",
    "        ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1741: .groupby() for grouping, and GroupBy.sum() for computing the sum of each group (preceded by desired columns to operate on), .reset_index(name=\"...\") to \"flatten\" the resulting statistics df and assign a name to the computed column. \n",
    "# Note that the column \"diff_time\" is automatically dropped upon completion of the computation, along with the \"in_time\" and \"out_time\" columns because they are in neither the by columns or being used to compute. \n",
    "def total_time(employees: pd.DataFrame) -> pd.DataFrame:\n",
    "    employees[\"diff_time\"] = employees[\"out_time\"] - employees[\"in_time\"]\n",
    "    employees_total_times_by_date = employees.groupby([\"event_day\", \"emp_id\"])[\"diff_time\"].sum().reset_index(name=\"total_time\").rename(columns={\"event_day\":\"day\"})\n",
    "    \n",
    "    return employees_total_times_by_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 511: Groupby.min()\n",
    "# similarly to the above 1741, doing [\"event_date\"] makes it so only columns player_id (the by) and event_date (to be renamed first_login) are present\n",
    "def game_analysis(activity: pd.DataFrame) -> pd.DataFrame:\n",
    "    first_logins = activity.groupby(by=\"player_id\")[\"event_date\"].min().reset_index(name=\"first_login\")\n",
    "    return first_logins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2356: Groupby.nunique() \n",
    "def count_unique_subjects(teacher: pd.DataFrame) -> pd.DataFrame:\n",
    "    subject_counts = teacher.groupby(by=[\"teacher_id\"])[\"subject_id\"].nunique().reset_index(name=\"cnt\")\n",
    "    return subject_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 596: Groupby.count() counts only non-null entries. use for when unconcerned about duplicates\n",
    "def find_classes(courses: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Groupby.count() counts only non-null entries\n",
    "    course_counts = courses.groupby(by=[\"class\"])[\"student\"].count().reset_index().rename(columns={\"student\": \"n_students\"})\n",
    "    return course_counts[course_counts[\"n_students\"] >= 5][[\"class\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 586: going with the trend in this section, there's nothing really special here. but we can alternatively use DataFrame.mode() which is an inbuilt function for this exact purpose -- finding the entry with \n",
    "def largest_orders(orders: pd.DataFrame) -> pd.DataFrame:\n",
    "    order_counts = orders.groupby(\"customer_number\")[\"order_number\"].count().reset_index(name=\"n_orders\")\n",
    "    max_orders_by_customer = order_counts[\"n_orders\"].max()\n",
    "    customer_w_most_orders = order_counts[order_counts[\"n_orders\"] == max_orders_by_customer]\n",
    "    return customer_w_most_orders[[\"customer_number\"]]\n",
    "\n",
    "def largest_orders(orders: pd.DataFrame) -> pd.DataFrame:\n",
    "    # here, .mode() returned a Series\n",
    "    return orders[\"customer_number\"].mode().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1484: .agg() to apply function(s) to each column of each group; in general, each column may receive its own independent set of functions.  .agg() returns a df (indexed up) where all groups are visible along with their rows.\n",
    "# in this case, we apply nunique and a string concatenation on the lone column in each group \"product\", generating two new columns in the resulting df, one for nunique and one for the string concatenation. \n",
    "def categorize_products(activities: pd.DataFrame) -> pd.DataFrame:\n",
    "    # note that agg runs each function on each group for each column unless specified otherwise with a dict, in this case only column \"product\". Each of the generated \"nunique\" and \"<lambda_0>\" columns are housed under what seems to be what I call a \"supercolumn\", housing both of them along with the by column; essentially housing all of the groups; https://imgur.com/a/rH5Rd4p. So we access it with [\"product\"].  \n",
    "    products_and_n_by_date = activities.groupby(\"sell_date\").agg({\"product\": [\"nunique\", lambda col: \",\".join(sorted(col.unique()))]})[\"product\"].reset_index().rename(columns={\"nunique\": \"num_sold\", \"<lambda_0>\": \"products\"}) # for some reason, name= is not expected in .reset_index() here. something screwy with the levels. \n",
    "\n",
    "    return products_and_n_by_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1693: .agg() with particular functions for each column in each group, and custom naming of resulting columns\n",
    "def daily_leads_and_partners(daily_sales: pd.DataFrame) -> pd.DataFrame:\n",
    "    # alternatively, use .nunique() instead of .agg(), then .rename(), as shown below. \n",
    "    return daily_sales.groupby(by=[\"date_id\", \"make_name\"]).agg(unique_leads=(\"lead_id\", \"nunique\"), unique_partners=(\"partner_id\", \"nunique\")).reset_index()\n",
    "\n",
    "def daily_leads_and_partners(daily_sales: pd.DataFrame) -> pd.DataFrame:\n",
    "    # for some reason, name= is not expected in .reset_index() here. In the cases where it was allowed, we preceded the statistic function with a column name. I tried to do the preceding and including names=[\"unique_leads\", and \"partner_id\"], but this strangely renamed the first two columns only. \n",
    "    return daily_sales.groupby(by=[\"date_id\", \"make_name\"]).nunique().reset_index().rename(columns={\"lead_id\": \"unique_leads\", \"partner_id\": \"unique_partners\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1050: .query() for filtering without having to make a new variable name (in Approach 3)\n",
    "# (this is classified under Data Integration, but it fits more in Aggregation imho)\n",
    "def actors_and_directors(actor_director: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Approach 1: .filter(function) filters all elements of each group and aggregates it all back into a dataframe. \n",
    "    return actor_director.groupby([\"actor_id\", \"director_id\"]).filter(lambda df: df.shape[0] >= 3).drop(columns=[\"timestamp\"]).drop_duplicates()\n",
    "    \n",
    "def actors_and_directors(actor_director: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Approach 2: .agg()\n",
    "    num_coops = actor_director.groupby([\"actor_id\", \"director_id\"]).agg(count=(\"timestamp\", \"count\")).reset_index()\n",
    "    coops_3_or_more = num_coops[num_coops[\"count\"] >= 3]\n",
    "    return coops_3_or_more.drop(columns=[\"count\"])\n",
    "\n",
    "def actors_and_directors(actor_director: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Approach 3: .count() and .query() \n",
    "    return actor_director.groupby([\"actor_id\", \"director_id\"])[\"timestamp\"].count().reset_index(name=\"count\").query(\"count >= 3\").drop(columns=[\"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1378: pd.merge() -- left join to perform a vlookup and keep values in the left table that did not find a match; match them with None\n",
    "def replace_employee_id(employees: pd.DataFrame, employee_uni: pd.DataFrame) -> pd.DataFrame:\n",
    "  return pd.merge(left=employees, right=employee_uni, on=[\"id\"], how=\"left\")[[\"unique_id\", \"name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1280: cartesian product .merge()/join; .reset_index(name=\"...\"); .fillna(); .sort_values() by two+ columns\n",
    "# doing GroupBy.count() without the preceding [\"subject_name\"] counts nothing, since each group contains only the by columns \"student_id\" and \"subject_name\" and no value columns; this can be verified with .get_group((1,\"Math\")). hence, you receive no new columns from doing .count().  hence, specifying to use .count() on \"subject_name\" is necessary and in the resulting df, each primary key (student_id, subject_name) is assigned their counts. moreover, for some reason the count column is not assigned a name, so we specify a name in .reset_index()\n",
    "def students_and_examinations(students: pd.DataFrame, subjects: pd.DataFrame, examinations: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    template = pd.merge(students, subjects, how=\"cross\")\n",
    "    exam_counts = examinations.groupby(by=[\"student_id\", \"subject_name\"])[\"subject_name\"].count().reset_index(name=\"attended_exams\") # [1]\n",
    "    exam_counts_in_template = pd.merge(template, exam_counts, how=\"left\", on=[\"student_id\", \"subject_name\"]).fillna(0).sort_values([\"student_id\", \"subject_name\"])\n",
    "\n",
    "    return exam_counts_in_template[[\"student_id\", \"student_name\", \"subject_name\", \"attended_exams\"]] # specifying the column orders at the end is just for the dumb edge case where examinations and subject lists are empty; in this case, for some reason the student_id and student_name columns are swapped. \n",
    "\n",
    "# [1] # alternatively, exam_counts = examinations.groupby(by=[\"student_id\", \"subject_name\"]).agg(attended_exams=(\"subject_name\", \"count\")).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 570: .isin() to generate a boolean mask\n",
    "def find_managers(employee: pd.DataFrame) -> pd.DataFrame:\n",
    "    manager_ids_w_5_plus = employee.groupby(by=[\"managerId\"])[\"id\"].count().reset_index(name=\"n_reports\").query(\"n_reports >= 5\")[[\"managerId\"]]\n",
    "    is_manager_w_5_plus = employee[\"id\"].isin(manager_ids_w_5_plus[\"managerId\"])\n",
    "    manager_names_w_5_plus = employee[is_manager_w_5_plus][[\"name\"]]\n",
    "\n",
    "    return manager_names_w_5_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 607: \n",
    "def sales_person(sales_person: pd.DataFrame, company: pd.DataFrame, orders: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    sales_people_w_red = pd.merge(orders, company[company[\"name\"] == \"RED\"], on=\"com_id\")[\"sales_id\"].unique() # changing == to != will NOT get the sales_people_wo_red. All this does is drop the rows with red in them. we have to filter each sales person. \n",
    "    has_worked_w_red = sales_person[\"sales_id\"].isin(sales_people_w_red)\n",
    "    sales_people_wo_red = sales_person[~has_worked_w_red]\n",
    "\n",
    "    return sales_people_wo_red[[\"name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
