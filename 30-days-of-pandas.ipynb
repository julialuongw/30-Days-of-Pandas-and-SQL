{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- 30 Days of Pandas Challenge: Solutions with concepts review ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>population</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>25500100</td>\n",
       "      <td>652230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>37100000</td>\n",
       "      <td>2381741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name  population     area\n",
       "0  Afghanistan    25500100   652230\n",
       "2      Algeria    37100000  2381741"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 595: or-filtering\n",
    "\n",
    "data = [['Afghanistan', 'Asia', 652230, 25500100, 20343000000], ['Albania', 'Europe', 28748, 2831741, 12960000000], ['Algeria', 'Africa', 2381741, 37100000, 188681000000], ['Andorra', 'Europe', 468, 78115, 3712000000], ['Angola', 'Africa', 1246700, 20609294, 100990000000]]\n",
    "world = pd.DataFrame(data, columns=['name', 'continent', 'area', 'population', 'gdp']).astype({'name':'object', 'continent':'object', 'area':'Int64', 'population':'Int64', 'gdp':'Int64'})\n",
    "\n",
    "def big_countries(world: pd.DataFrame) -> pd.DataFrame:\n",
    "    big_countries = world[(world[\"area\"] >= 3000000) | (world[\"population\"] >= 25000000)]\n",
    "    return big_countries[[\"name\",\"population\",\"area\"]]\n",
    "\n",
    "big_countries(world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1757: and-filtering\n",
    "\n",
    "def find_products(products: pd.DataFrame) -> pd.DataFrame:\n",
    "    return products[(products[\"low_fats\"] == \"Y\") & (products[\"recyclable\"] == \"Y\")][[\"product_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 183: isin filtering, negation\n",
    "\n",
    "def find_customers(customers: pd.DataFrame, orders: pd.DataFrame) -> pd.DataFrame:\n",
    "    does_not_order = ~customers[\"id\"].isin(orders[\"customerId\"])\n",
    "    return customers[does_not_order][[\"name\"]].rename(columns={\"name\":\"Customers\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1148: .drop_duplicates(), .sort_values()\n",
    "\n",
    "def article_views(views: pd.DataFrame) -> pd.DataFrame:\n",
    "    self_views = views[views[\"author_id\"] == views[\"viewer_id\"]]\n",
    "    unique_self_views = self_views.drop_duplicates(subset=[\"author_id\", \"viewer_id\"])\n",
    "\n",
    "    return unique_self_views[[\"author_id\"]].rename(columns={\"author_id\": \"id\"}).sort_values(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1683: .len()\n",
    "def invalid_tweets(tweets: pd.DataFrame) -> pd.DataFrame:\n",
    "    return tweets[tweets[\"content\"].str.len() > 15][[\"tweet_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1873: .startswith(), .loc[mask, \"col\"]\n",
    "def calculate_special_bonus(employees: pd.DataFrame) -> pd.DataFrame:\n",
    "    employees[\"bonus\"] = 0\n",
    "    \n",
    "    # access the bonus field of each row that deserves a bonus\n",
    "    # loc is efficient to doing df[deserves_bonus][\"bonus\"], where an intermiate df gets produced. I guess .apply() is still nice too\n",
    "    deserves_bonus = ~(employees[\"name\"].str.startswith(\"M\")) & (employees[\"employee_id\"] % 2 == 1)\n",
    "    employees.loc[deserves_bonus, \"bonus\"] = employees[\"salary\"]\n",
    "    return employees[[\"employee_id\", \"bonus\"]].sort_values(\"employee_id\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1667: .lower(), .capitalize()\n",
    "def fix_names(users: pd.DataFrame) -> pd.DataFrame:\n",
    "    users[\"name\"] = users[\"name\"].str.lower().str.capitalize()\n",
    "    return users.sort_values(\"user_id\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1517: .apply() for more particular string conditions, no regex\n",
    "def valid_prefix(s: str) -> bool:\n",
    "    starts_with_letter = s[0].isalpha()\n",
    "    if not starts_with_letter:\n",
    "        return False\n",
    "    \n",
    "    valid_special_chars = set(\".-_\")\n",
    "    for char in s:\n",
    "        if (char in valid_special_chars) or char.isalpha() or char.isdigit():\n",
    "            continue\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def valid_emails(users: pd.DataFrame) -> pd.DataFrame:\n",
    "    valid_prefix_mask = users[\"mail\"].str.rsplit(\"@\", n=1).str[0].apply(valid_prefix)\n",
    "    valid_domain_mask = users[\"mail\"].str.endswith(\"@leetcode.com\")\n",
    "    return users[valid_prefix_mask & valid_domain_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1527: regex\n",
    "def find_patients(patients: pd.DataFrame) -> pd.DataFrame:\n",
    "    #has_diab1 = patients[\"conditions\"].str.split(' ').apply(lambda l: pd.Series(l).str.startswith(\"DIAB1\").any())\n",
    "    has_diab1 = patients[\"conditions\"].str.contains(r\"\\bDIAB1\\w*\\b\")\n",
    "    return patients[has_diab1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 177: pairing .sort_values() with .iloc[] to access nth highest entry\n",
    "def nth_highest_salary(employee: pd.DataFrame, N: int) -> pd.DataFrame:\n",
    "    # remove duplicate salaries to avoid ranking two duplicate salaries different rankings\n",
    "    employee = employee.drop_duplicates(\"salary\")\n",
    "\n",
    "    if N > len(employee):\n",
    "        return pd.DataFrame({'Nth Highest Salary': [None]})\n",
    "\n",
    "    nth_highest_salary = employee.sort_values(\"salary\", ascending=False)[\"salary\"].iloc[N-1]\n",
    "    return pd.DataFrame({'Nth Highest Salary': [nth_highest_salary]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 176: Jokes on you, we can use .nlargest() to return the n rows containing the nlargest. then use .iloc[] to select the desired 2nd highest\n",
    "def second_highest_salary(employee: pd.DataFrame) -> pd.DataFrame:\n",
    "    employee = employee.drop_duplicates(\"salary\")\n",
    "\n",
    "    if len(employee) < 2:\n",
    "        return pd.DataFrame({\"SecondHighestSalary\": [None]})\n",
    "    \n",
    "    # nlargest returns the rows containing the 1st and 2nd largest salaries\n",
    "    second_highest_salary = employee.nlargest(2, \"salary\").iloc[-1][\"salary\"]\n",
    "    return pd.DataFrame({\"SecondHighestSalary\": [second_highest_salary]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#184: .groupby() and .transform() to attribute group statistics to each entry in original df. could also have been accomplished with a merge as well after groupby.  \n",
    "def department_highest_salary(employee: pd.DataFrame, department: pd.DataFrame) -> pd.DataFrame:\n",
    "    employee[\"dept_highest_salary\"] = employee.groupby(\"departmentId\")[\"salary\"].transform(\"max\")\n",
    "    \n",
    "    is_highest_salary = employee[\"salary\"] == employee[\"dept_highest_salary\"]\n",
    "\n",
    "    employees_w_highest_salaries = pd.merge(employee[is_highest_salary], department, left_on=\"departmentId\", right_on=\"id\").rename(columns={\n",
    "        \"name_y\": \"Department\",\n",
    "        \"name_x\": \"Employee\",\n",
    "        \"salary\": \"Salary\"\n",
    "        })\n",
    "\n",
    "    return employees_w_highest_salaries[[\"Department\", \"Employee\", \"Salary\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 178: .rank() and using its \"method\" parameter to deal with tiebreakers\n",
    "def order_scores(scores: pd.DataFrame) -> pd.DataFrame:\n",
    "    scores = scores.sort_values(\"score\", ascending=False)\n",
    "    scores[\"rank\"] = scores[\"score\"].rank(ascending=False, method=\"dense\")\n",
    "    return scores.drop(columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 196: using the \"inplace\" parameter, .drop_duplicates() with \"keep\" parameter to deal with tiebreakers\n",
    "# Modify Person in place\n",
    "def delete_duplicate_emails(person: pd.DataFrame) -> None:\n",
    "    person.sort_values(\"id\", ascending=True, inplace=True)\n",
    "    person.drop_duplicates(subset=[\"email\"], keep=\"first\", inplace=True) # \"first\" to keep the row with smallest id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1795: .melt() to convert wide data to long data, .dropna() to drop rows contaning None\n",
    "def rearrange_products_table(products: pd.DataFrame) -> pd.DataFrame:\n",
    "    # id_vars: the pivot column, \n",
    "    # var_name: the name of the new column holding the \"wide\" columns, \n",
    "    # value_name: since you're migrating the wide columns to its own column under var_name, this value_name will be the name of the new column name for the values. \n",
    "    products_long = products.melt(id_vars=\"product_id\", var_name=\"store\", value_name=\"price\")\n",
    "    products_long_nonulls = products_long.dropna()\n",
    "    return products_long_nonulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2082: .nunique() to count number of distinct entries\n",
    "def count_rich_customers(store: pd.DataFrame) -> pd.DataFrame:\n",
    "    filtered_store = store[store[\"amount\"] > 500]\n",
    "    n_rich_customers = filtered_store[\"customer_id\"].nunique()\n",
    "    \n",
    "    return pd.DataFrame({\"rich_count\": [n_rich_customers]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1173: calculating percentages, using .shape[0] to access number of rows \n",
    "def food_delivery(delivery: pd.DataFrame) -> pd.DataFrame:\n",
    "    is_same_day = delivery[\"order_date\"] == delivery[\"customer_pref_delivery_date\"]\n",
    "    prop_intermediate = round(100 * delivery[is_same_day].shape[0] / delivery.shape[0], 2)\n",
    "    return pd.DataFrame({\"immediate_percentage\": [prop_intermediate]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1907: classification, no new methods\n",
    "def count_salary_categories(accounts: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.DataFrame({\n",
    "        \"category\": [\"Low Salary\", \"Average Salary\", \"High Salary\"],\n",
    "        \"accounts_count\": [\n",
    "            accounts[ accounts[\"income\"] < 20000 ].shape[0],\n",
    "            accounts[ (20000 <= accounts[\"income\"]) & (accounts[\"income\"] <= 50000) ].shape[0],\n",
    "            accounts[ 50000 < accounts[\"income\"] ].shape[0]\n",
    "        ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1741: .groupby() for grouping, and GroupBy.sum() for computing the sum of each group (preceded by desired columns to operate on), .reset_index() to \"flatten\" the resulting statistics df\n",
    "def total_time(employees: pd.DataFrame) -> pd.DataFrame:\n",
    "    employees[\"total_time\"] = employees[\"out_time\"] - employees[\"in_time\"]\n",
    "    employees_total_times_by_date = employees.groupby([\"event_day\", \"emp_id\"])[\"total_time\"].sum().reset_index()\n",
    "    \n",
    "    return employees_total_times_by_date.rename(columns={\"event_day\":\"day\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 511: Groupby.min()\n",
    "def game_analysis(activity: pd.DataFrame) -> pd.DataFrame:\n",
    "    # doing [\"event_date\"] makes it so only columns player_id (the by) and event_date (to be renamed first_login) are present\n",
    "    first_logins = activity.groupby(by=\"player_id\")[\"event_date\"].min().reset_index().rename(columns={\"event_date\": \"first_login\"})\n",
    "    return first_logins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2356: Groupby.nunique() \n",
    "def count_unique_subjects(teacher: pd.DataFrame) -> pd.DataFrame:\n",
    "    # similarly to Groupby.min(), [\"subject_id\"].nunique() here leaves only the \"teacher_id\" (the by) and \"subject_id\" (to be renamed cnt) columns in the resulting df\n",
    "    subject_counts = teacher.groupby(by=[\"teacher_id\"])[\"subject_id\"].nunique().reset_index().rename(columns={\"subject_id\": \"cnt\"})\n",
    "    return subject_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 596: Groupby.count() counts only non-null entries. use for when unconcerned about duplicates\n",
    "def find_classes(courses: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Groupby.count() counts only non-null entries\n",
    "    course_counts = courses.groupby(by=[\"class\"])[\"student\"].count().reset_index().rename(columns={\"student\": \"n_students\"})\n",
    "    return course_counts[course_counts[\"n_students\"] >= 5][[\"class\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 586: going with the trend in this section, there's nothing really special here. but we can alternatively use DataFrame.mode() which is an inbuilt function for this exact purpose -- finding the entry with \n",
    "def largest_orders(orders: pd.DataFrame) -> pd.DataFrame:\n",
    "    order_counts = orders.groupby(\"customer_number\")[\"order_number\"].count().reset_index().rename(columns={\"order_number\": \"n_orders\"})\n",
    "    max_orders_by_customer = order_counts[\"n_orders\"].max()\n",
    "    customer_w_most_orders = order_counts[order_counts[\"n_orders\"] == max_orders_by_customer]\n",
    "    return customer_w_most_orders[[\"customer_number\"]]\n",
    "\n",
    "def largest_orders(orders: pd.DataFrame) -> pd.DataFrame:\n",
    "    # here, .mode() returned a Series\n",
    "    return orders[\"customer_number\"].mode().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1484: .agg() to apply function(s) to each column of each group. in this case, we apply nunique and a string concatenation on the lone column in each group \"product\", generating two new columns in the resulting df, one for nunique and one for the string concatenation. \n",
    "def categorize_products(activities: pd.DataFrame) -> pd.DataFrame:\n",
    "    # note that agg runs each function on each group for each column. so we extract \"product\" column at the end\n",
    "    ret_df = activities.groupby(\"sell_date\").agg([\"nunique\", lambda col: \",\".join(sorted(col.unique()))])[\"product\"].reset_index()\n",
    "    \n",
    "    return ret_df.rename(columns={\"nunique\": \"num_sold\", \"<lambda_0>\": \"products\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1693: .agg() with particular functions for each column in each group. \n",
    "def daily_leads_and_partners(daily_sales: pd.DataFrame) -> pd.DataFrame:\n",
    "    # alternatively, use .nunique() instead of .agg(), then .rename()\n",
    "    return daily_sales.groupby(by=[\"date_id\", \"make_name\"]).agg(unique_leads=(\"lead_id\", \"nunique\"), unique_partners=(\"partner_id\", \"nunique\")).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1050: \n",
    "# (this is classified under Data Integration, but it fits more in Aggregation imho)\n",
    "def actors_and_directors(actor_director: pd.DataFrame) -> pd.DataFrame:\n",
    "    # This also works. .filter(function) filters all elements of each group and aggregates it all back into a dataframe. \n",
    "    #return actor_director.groupby([\"actor_id\", \"director_id\"]).filter(lambda df: df.shape[0] >= 3).drop(columns=[\"timestamp\"]).drop_duplicates()\n",
    "    \n",
    "    # keep in mind .agg returns a df (indexed up) where all groups are visible along with their rows \n",
    "    num_coops = actor_director.groupby([\"actor_id\", \"director_id\"]).agg(count=(\"timestamp\", \"count\")).reset_index()\n",
    "    coops_3_or_more = num_coops[num_coops[\"count\"] >= 3]\n",
    "    return coops_3_or_more.drop(columns=[\"count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1378: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
